{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machines\n",
    "\n",
    "**Data** [Gender-annoted dataset of European parliament talks](https://www.kaggle.com/ellarabi/europarl-annotated-for-speaker-gender-and-age)\n",
    "\n",
    "**Overreaching question** Can we develop a model which correctly predicts speakers' gender, based on what they are saying?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data management\n",
    "\n",
    "Let's create a dataset with the variable of interest and the textual data.\n",
    "The data about gender is stored as XML, so we need to do a bit of work before we can easily use it.\n",
    "The below code also transforms the text data into a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = open('./data/europarl-annotated-for-speaker-gender-and-age/europarl.de-en/europarl.de-en.dat').readlines()\n",
    "all_texts = open('./data/europarl-annotated-for-speaker-gender-and-age/europarl.de-en/europarl.de-en.en.aligned.tok').readlines()\n",
    "\n",
    "## Check that both files have the same number of rows\n",
    "assert len(metadata) == len(all_texts)\n",
    "\n",
    "## Processign the data takes some time, so let's choose a random set of 1000 messages to try initial modeling\n",
    "\n",
    "import random\n",
    "random.seed(1) # Set seed for reproducible results\n",
    "\n",
    "selected_lines = random.sample( range( len( metadata ) ) , k = 1000 )\n",
    "\n",
    "print( metadata[0] )\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "genders = []\n",
    "selected_texts = []\n",
    "\n",
    "# Parse metadata\n",
    "for line in selected_lines:\n",
    "    \n",
    "    md = BeautifulSoup( metadata[ line ] )\n",
    "    genders.append( md.line['gender'] )\n",
    "    \n",
    "    selected_texts.append( all_texts[ line ] )\n",
    "    \n",
    "\n",
    "print( len( genders ) )\n",
    "print( len( selected_texts ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer()\n",
    "document_term_matrix = tf_vectorizer.fit_transform( selected_texts )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train-test split\n",
    "\n",
    "Used later in the analysis to ensure we do not [overfit](https://en.wikipedia.org/wiki/Overfitting) to the data when training the classifier. Let's use 20% of data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_train, label_test, data_train, data_test = train_test_split( genders, document_term_matrix, test_size = .2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and evaluate SVM classifier\n",
    "\n",
    "We now train the model using the **training** data and measure its performance using the **test** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC(kernel='linear') # Linear Kernel, default settings\n",
    "model.fit( data_train, label_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "## Check how well the model predicts test data\n",
    "label_test_pred = model.predict( data_test )\n",
    "print( metrics.accuracy_score( label_test, label_test_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the importance of different words for the predictions\n",
    "\n",
    "predictors = {}\n",
    "\n",
    "for i, name in enumerate( tf_vectorizer.get_feature_names_out() ):\n",
    "    predictors[name] = i\n",
    "    \n",
    "    \n",
    "for name, value in predictors.items():\n",
    "    predictors[name] = model.coef_[0, value ]\n",
    "    \n",
    "\n",
    "print( predictors )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try\n",
    "\n",
    "* Run the above code as is and interprent the accuracy. What does the score mean?\n",
    "* Examine different metrics for [classification accuracy](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics).\n",
    "* Fix issues in the text pre-processing. Account for stop words, frequent terms and stem content in the document-term-matrix. Does this have any infuence on the model's accuracy?\n",
    "* Predictors include each feature in data (i.e., term), and how important they were in predicting the data. Extract and inspect the best predictor features.\n",
    "* Modify the code to use [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) model instead of SVM. Which model seems to work better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's now try to improve the model's performance through *tuning* its parameters.\n",
    "* [Grid search](https://scikit-learn.org/stable/modules/grid_search.html) is an approach to systematically assess the performance different modeling parameter values.\n",
    "* You can also work on preprocessing to [scale](https://scikit-learn.org/stable/modules/preprocessing.html) the data, or try more acressive cleaning or removal of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameter range for different models\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "many_models = GridSearchCV( svm.SVC(), param_grid )\n",
    "many_models.fit( data_train, label_train )\n",
    "\n",
    "print( many_models )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameter after tuning \n",
    "print(many_models.best_params_) \n",
    "  \n",
    "# Print how our model looks after hyper-parameter tuning \n",
    "print(many_models.best_estimator_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check how well the best model predicts\n",
    "label_test_pred = many_models.predict( data_test )\n",
    "print( metrics.accuracy_score( label_test, label_test_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have so far used a binary variable (male/female) as target. However, support vector machines can be used to perform [multi-category classification](https://scikit-learn.org/stable/modules/svm.html#multi-class-classification) or to use [linear variables through regression models](https://scikit-learn.org/stable/modules/svm.html#regression).\n",
    "\n",
    "* If doing multi-category classification, the algorithm is senstive to inbalances between classes, i.e. if there are more cases belonging to Category 1 than in Category 2.\n",
    "\n",
    "* This can be fixed through weighting to balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='linear', class_weight='balanced') # Linear Kernel, default settings\n",
    "model.fit( data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check how well we did for testing data\n",
    "label_test_pred = model.predict( data_test )\n",
    "print( metrics.accuracy_score( label_test, label_test_pred ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try\n",
    "\n",
    "* Try different grid search parameters, see if your accuracy metric improve.\n",
    "* Does balancing improve accuracy with our data?\n",
    "* Use age variable to develop a regression model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
