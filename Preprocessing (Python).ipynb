{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d1b10c",
   "metadata": {},
   "source": [
    "# Preprocessing text data\n",
    "\n",
    "The purpose of this notebook is to try out different preprocessing steps for text data, and to see how changes in preprocessing can influence the data that gets input to modeling.\n",
    "\n",
    "## Dataset for the exercise\n",
    "\n",
    "* [New York Times Comments](https://www.kaggle.com/aashita/nyt-comments/data)  <-  set of readers' comments to articles published in the New York Times.\n",
    "\n",
    "## Tools\n",
    "\n",
    "Python has a variety of preprocessing tools. This example uses some of the tools built into [scikit-learn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) and the [nltk package](https://www.nltk.org/). However the same steps could be performed using other tools as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6c689",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f618de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcfcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/nyt-comments/'\n",
    "files = os.listdir( path ) ## Get all files from directory path\n",
    "files = [f for f in files if 'Comments' in f] ## Get only files with reader comments\n",
    "\n",
    "# For the purposes of the example, let's use only one of the data files\n",
    "file = files[0]\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Iterate over entries in data and add their comment body to list\n",
    "for entry in csv.DictReader( open( path + file ) ):\n",
    "    documents.append( entry['commentBody'] )\n",
    "\n",
    "print( \"Data size\" )\n",
    "print( len(documents) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ce8f9",
   "metadata": {},
   "source": [
    "## Preprocess and create Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fba1d2f",
   "metadata": {},
   "source": [
    "We try here several basic preprocessing steps, including removing html tags, removing punctuation, removing numbers, removing stopwords, lowercasing and stemming words, and finally removing infrequent and very frequent words. Each of these have implications for the resulting Document-Term Matrix. You can try out different options below and see their influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d103a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Let's use nltk's in-built stopword list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Add to or replace this list to use custom stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdcd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "\n",
    "# Function to perform basic preprocessing\n",
    "def preprocess( doc ):\n",
    "    \n",
    "    # Remove html\n",
    "    p = re.compile(r'<.*?>')\n",
    "    doc = p.sub('', doc)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    doc = doc.translate( str.maketrans( str.maketrans('', '', string.punctuation) ) )\n",
    "    \n",
    "    # Remove numbers\n",
    "    doc = doc.translate( str.maketrans('', '', digits) )\n",
    "    \n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    doc = re.sub(' +', ' ', doc) \n",
    "    doc = doc.strip()\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b12439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try preprocessing on one document\n",
    "print( documents[0] )\n",
    "print( preprocess( documents[0] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214af3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess both stopwords and actual data using same steps\n",
    "documents = [preprocess(doc) for doc in documents]\n",
    "stopwords = [preprocess(stop) for stop in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "stemmer = EnglishStemmer() # nltk's in-built stemmer\n",
    "\n",
    "# Function for stemming texts\n",
    "def stem( text ):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return [ stemmer.stem(w) for w in words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f43df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words – this might take some time!\n",
    "documents_stemmed = [' '.join( stem(d) ) for d in documents]\n",
    "stopwords_stemmed = stem( ' '.join( stopwords ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb127e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.90, # Remove words that occur in over 90% of the documents\n",
    "    min_df=10, # Remove words that occur in less than 10 documents\n",
    "    stop_words=stopwords_stemmed, # Remove stop words\n",
    "    analyzer = \"word\"\n",
    ")\n",
    "\n",
    "# Create the Document-Term Matrix\n",
    "dtm = tf_vectorizer.fit_transform(documents_stemmed)\n",
    "\n",
    "# Get list of words in the DTM\n",
    "list( tf_vectorizer.get_feature_names_out() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84826c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Transform DTM to pandas dataframe to examine word frequencies\n",
    "dtm_df = pd.DataFrame( dtm.todense() )\n",
    "dtm_df.columns = tf_vectorizer.get_feature_names_out()\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = dtm_df.sum() # Get word frequencies for each term in vocabulary\n",
    "doc_counts = dtm_df.astype(bool).sum() # Get document frequencies for each term in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most frequent words, based on word count\n",
    "word_counts.sort_values( ascending=False ).nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0913f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most frequent words, based on document frequency\n",
    "doc_counts.sort_values( ascending=False ).nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85910968",
   "metadata": {},
   "source": [
    "## Things to try out and think about\n",
    "\n",
    "* Check the top words and list of words in the Document-Term Matrix. Do you see anything that should still be removed?\n",
    "* Modify the stopword list to remove unwanted words\n",
    "* Think about different ways of performing preprocessing and try these. Should you e.g. replace punctuation with whitespace, or remove it altogether, as in the above code? Should numbers be removed? Try also different thresholds for removing terms on the basis of document frequency."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
