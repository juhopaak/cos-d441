{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic models and LDA\n",
    "\n",
    "## Dataset for the exercise\n",
    "\n",
    "* [New York Times Comments](https://www.kaggle.com/aashita/nyt-comments/data)  <-  set of readers' comments to articles published in the New York Times.\n",
    "\n",
    "## Overarching research question\n",
    "\n",
    "The comments provide a perspective to the kinds of concerns people in discussions related to online articles.\n",
    "What kind of meaningful themes - if any - emerge from this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sample size 21825\n"
     ]
    }
   ],
   "source": [
    "## Data collection from files.\n",
    "## To keep the dataset fairly small, we conduct random data selection here.\n",
    "## This is *ONLY* for teaching purposes, to ensure that the model runs relatively fast.\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "random.seed(1) # Set random seed for reproducible results\n",
    "\n",
    "path = 'data/nyt-comments/'\n",
    "files = os.listdir( path ) ## Get all files from directory path\n",
    "files = filter( lambda file_name: file_name.startswith(\"Comments\"), files )\n",
    "files = map( lambda file_name: path + file_name, files ) ## Add path to file names\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file in files:\n",
    "    for entry in csv.DictReader( open( file ) ):\n",
    "        \n",
    "        if random.random() > .99: ## Choose content randomly\n",
    "            comment = entry['commentBody']\n",
    "\n",
    "            documents.append( comment )\n",
    "            \n",
    "            \n",
    "print(\"Data sample size\", len(documents) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From text data to document-term matrix\n",
    "\n",
    "To analyse textual data we transform them to a document term matrix, where rows correspond to documents (= reader comments) and columns correspond to words in the dataset.\n",
    "\n",
    "Note how we **preprocess** below the texts for analysis. We remove stopwords (through a set of common English stopwords; we could also create our own lists), stem the content of comments to ensure language is treated well and lowercase everything in the content. Thus, the `document_terms` that preprocessing produces is a huge sparse matrix in the end. Preprocessing is its own kind of art, as it can [influence results](https://www.cambridge.org/core/product/identifier/S1047198717000444/type/journal_article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juhopaak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "# Let's use nltk's in-built stopword list and stemmer\n",
    "nltk.download('stopwords')\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "# Add to or replace this list to use custom stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Function for stemming texts\n",
    "def stem( text ):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return [ stemmer.stem(w) for w in words ]\n",
    "\n",
    "# Stem both documents and stopwords\n",
    "documents_stemmed = [' '.join( stem(d) ) for d in documents]\n",
    "stopwords_stemmed = stem( ' '.join( stopwords ) )\n",
    "\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.90, min_df=10, \n",
    "    stop_words=stopwords_stemmed, analyzer = \"word\", lowercase = True\n",
    ")\n",
    "\n",
    "document_terms = tf_vectorizer.fit_transform(documents_stemmed)\n",
    "document_terms_names = tf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From document-term matrix to analysis\n",
    "\n",
    "Finally we run the Latent Dirichlet Allocation process to the document-term matrix to create topics.\n",
    "Similarly to k-means, we need to choose the number of topics; there are also other parameters which could be used to _fine tune_ topic models, see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) for details.\n",
    "However, [topic models work on a different abstration level than humans](http://doi.wiley.com/10.1002/asi.23786) and thus interpretation and validation of the results is always needed when using the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation( n_components = 5 )\n",
    "model = lda.fit( document_terms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "\t br\n",
      "\t peopl\n",
      "\t year\n",
      "\t get\n",
      "\t work\n",
      "Topic 2\n",
      "\t trump\n",
      "\t br\n",
      "\t republican\n",
      "\t presid\n",
      "\t vote\n",
      "Topic 3\n",
      "\t br\n",
      "\t peopl\n",
      "\t us\n",
      "\t state\n",
      "\t use\n",
      "Topic 4\n",
      "\t br\n",
      "\t com\n",
      "\t war\n",
      "\t www\n",
      "\t gun\n",
      "Topic 5\n",
      "\t br\n",
      "\t trump\n",
      "\t like\n",
      "\t one\n",
      "\t peopl\n"
     ]
    }
   ],
   "source": [
    "for topic_number, words in enumerate( model.components_ ):\n",
    "        print( \"Topic\", topic_number+1 )\n",
    "        for word in words.argsort()[:-6:-1]:\n",
    "            print( \"\\t\", document_terms_names[word] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0052428 , 0.00526227, 0.16520212, 0.00519906, 0.81909375]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the distribution of topics in a single document\n",
    "model.transform( document_terms[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* If the model terms seem to contain unwanted words or characters, rerun preprocessing to remove these.\n",
    "* Compute the distribution of each topic for each document. Where could you use this?\n",
    "* Modify the code and examine a few potential topic numbers. What differences can you detect?\n",
    "* Modify the preprocessing to remove all words which shorter than four characters. What do you learn now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "There are many different approaches to evaluating topic models (see, [1](http://doi.acm.org/10.1145/1553374.1553515), [2](https://journal.fi/politiikka/article/view/79629) for examples).\n",
    "We can evaluate the suitability of topic models using statistical measurements such as loglikelihood, but [some say](http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf) that this might be bad practice - and [others](https://journal.fi/politiikka/article/view/79629) recommend it.\n",
    "You can get the loglikelihood for a model by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5872652.345350089"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score( document_terms )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Evaluate a set of different models based on loglikelihood. Which one would you choose?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
