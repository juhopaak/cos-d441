{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5266b30",
   "metadata": {},
   "source": [
    "# Preprocessing text data\n",
    "\n",
    "The purpose of this notebook is to try out different preprocessing steps for text data, and to see how changes in preprocessing can influence the data that gets input to modeling.\n",
    "\n",
    "## Dataset for the exercise\n",
    "\n",
    "* [New York Times Comments](https://www.kaggle.com/aashita/nyt-comments/data)  <-  set of readers' comments to articles published in the New York Times.\n",
    "\n",
    "## Tools\n",
    "\n",
    "R has a variety of preprocessing tools, including the popular libraries [tm](https://cran.r-project.org/web/packages/tm/tm.pdf) and [quanteda](https://cran.r-project.org/web/packages/quanteda/quanteda.pdf). This example uses quanteda - however the same steps could be performed using other tools as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff462d0",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6547c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path <- 'data/nyt-comments/'\n",
    "files <- list.files( path ) ## Get all files from directory path\n",
    "files <- files[ grepl(\"Comments\", files) ] ## Get only files with reader comments\n",
    "\n",
    "# For the purposes of the example, let's use only one of the comment data files\n",
    "data <- read.csv( paste(path, files[1],  sep='') )\n",
    "\n",
    "print( \"Data size\" )\n",
    "print( nrow(data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645b3b5",
   "metadata": {},
   "source": [
    "## Preprocess and create Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69049d2",
   "metadata": {},
   "source": [
    "We try here several basic preprocessing steps, including removing html tags, removing punctuation, removing numbers, removing stopwords, lowercasing and stemming words, and finally removing infrequent and frequent words. Each of these have implications for the resulting Document-Term Matrix. You can try out different options below and see their influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c49189",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(quanteda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bdde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data$commentBody <- gsub(\"<.*?>\", \"\", data$commentBody) # Remove html tags before creating corpus\n",
    "\n",
    "corp <- corpus( data, text_field = \"commentBody\" ) # Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad31d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords('en') ## Add to or replace this list to use custom stopwords\n",
    "\n",
    "# Split texts to word tokens\n",
    "token <- tokens( \n",
    "    corp, \n",
    "    remove_punct=TRUE, # Remove punctuation\n",
    "    remove_numbers=TRUE # Remove numbers\n",
    ")\n",
    "\n",
    "token <- tokens_tolower( token ) # Lowercase words\n",
    "token <- tokens_select( token, pattern = stopwords, selection = 'remove') # Remove stopwords before stemming\n",
    "token <- tokens_wordstem( token ) # Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DTM (quanteda calls it 'dfm' for 'Document-Feature Matrix')\n",
    "dtm <- dfm( token )\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f834712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove infrequent words\n",
    "dtm <- dfm_trim( \n",
    "    dtm, \n",
    "    min_docfreq = 10, ## Remove words that occur in less than 10 documents \n",
    "    max_docfreq = dim(dtm)[1] * 0.9 # Remove words that occur in more than 90% of the documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4993f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topfeatures( dtm, scheme='count', n=10 ) # Get 10 most frequent words, based on word count\n",
    "topfeatures( dtm, scheme='docfreq', n=10 ) # Get 10 most frequent words, based on document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of words in the DTM\n",
    "featnames(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50169b78",
   "metadata": {},
   "source": [
    "## Things to try out and think about\n",
    "\n",
    "* Check the top words and list of words in the Document-Term Matrix. Do you see anything that should still be removed?\n",
    "* Modify the stopword list to remove unwanted words.\n",
    "* There might be some strings and symbols that quanteda does not recognize as e.g. punctuation or numbers. How would you go about removing these?\n",
    "* You can check the quanteda [documentation](https://cran.r-project.org/web/packages/quanteda/quanteda.pdf), and especially the tokens-function (p. 77) to see the available preprocessing options there. Using regular expressions with the [gsub-function](https://www.digitalocean.com/community/tutorials/sub-and-gsub-function-r) might also help, as with the html tags above."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
