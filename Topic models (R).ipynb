{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic models and LDA\n",
    "\n",
    "## Dataset for the exercise\n",
    "\n",
    "* [New York Times Comments](https://www.kaggle.com/aashita/nyt-comments/data)  <-  set of readers' comments to articles published in the New York Times.\n",
    "\n",
    "## Overarching research question\n",
    "\n",
    "The comments provide a perspective to the kinds of concerns people raise in discussions related to online articles.\n",
    "What kind of meaningful themes - if any - emerge from this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data collection from files.\n",
    "## To keep the dataset fairly small, we conduct random data selection here.\n",
    "## This is *ONLY* for teaching purposes, to ensure that the model runs relatively fast.\n",
    "\n",
    "path <- 'data/nyt-comments/'\n",
    "files <- list.files( path ) ## Get all files from directory path\n",
    "files <- files[ grepl(\"Comments\", files) ]\n",
    "files <- paste( path, files, sep = '') ## Add path to file names\n",
    "\n",
    "set.seed(1) # Set random seed for reproducible results.\n",
    "\n",
    "data <- data.frame()\n",
    "\n",
    "for( file in files ){\n",
    "    d <- read.csv( file )\n",
    "    data <- rbind( data, d ) ## TODO: This is a slow and poor method of doing this merging.\n",
    "}\n",
    "\n",
    "documents <- data[ runif( nrow(data) ) > .99, ] ## Choose content randomly\n",
    "            \n",
    "print(\"Data sample size\" )\n",
    "print( nrow(documents) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format comments as character strings\n",
    "documents$commentBody <- as.character( documents$commentBody )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From text data to document-term matrix\n",
    "\n",
    "To analyse textual data we transform them to a document term matrix, where rows correspond to documents (= reader comments) and columns correspond to words in the dataset.\n",
    "\n",
    "Note how we **preprocess** below the texts for analysis. We remove stopwords (through a set of common English stopwords; we could also create our own lists), stem the content of comments to ensure language is treated well and lowercase everything in the content. Thus, the `document_terms` that preprocessing produces is a huge sparse matrix in the end. Preprocessing is its own kind of art, as it can [influence results](https://www.cambridge.org/core/product/identifier/S1047198717000444/type/journal_article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(quanteda)\n",
    "\n",
    "# Add to or replace this list to use custom stopwords\n",
    "stopwords = stopwords('en')\n",
    "\n",
    "corp <- corpus( documents, text_field = \"commentBody\" ) # Create corpus\n",
    "\n",
    "token <- tokens( corp, remove_punct=TRUE ) # Remove punctuation\n",
    "token <- tokens_select( token, pattern = stopwords, selection = 'remove') # Remove stopwords\n",
    "token <- tokens_wordstem( token ) # Stem words\n",
    "\n",
    "document_terms <- dfm( token )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From document-term matrix to analysis\n",
    "\n",
    "Finally we run the Latent Dirichlet Allocation process to the document-term matrix to create topics.\n",
    "Similarly to k-means, we need to choose the number of topics; there are also other parameters which could be used to _fine tune_ topic models, see [documentation](https://www.rdocumentation.org/packages/topicmodels/) for details.\n",
    "However, [topic models work on a different abstration level than humans](http://doi.wiley.com/10.1002/asi.23786) and thus interpretation and validation of the results is always needed when using the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(topicmodels)\n",
    "\n",
    "document_terms <- convert( document_terms, to = \"topicmodels\")\n",
    "model <- LDA( document_terms, k = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model terms\n",
    "terms( model, 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the distribution of topics in a single document\n",
    "posterior( model )$topics[1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* If the model terms seem to contain unwanted words or characters, rerun preprocessing to remove these.\n",
    "* Compute the distribution of each topic for each document. Where could you use this?\n",
    "* Modify the code and examine a few potential topic numbers. What differences can you detect?\n",
    "* Modify the preprocessing to remove all words which shorter than four characters. What do you learn now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "There are many different approaches to evaluating topic models (see, [1](http://doi.acm.org/10.1145/1553374.1553515), [2](https://journal.fi/politiikka/article/view/79629) for examples).\n",
    "We can evaluate the suitability of topic models using statistical measurements such as loglikelihood, but [some say](http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf) that this might be bad practice - and [others](https://journal.fi/politiikka/article/view/79629) recommend it.\n",
    "You can get the loglikelihood for a model by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLik( model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Evaluate a set of different models based on loglikelihood. Which one would you choose?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
