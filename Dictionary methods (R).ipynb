{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary methods\n",
    "\n",
    "## Dataset for the exercise\n",
    "\n",
    "* [New York Times Comments](https://www.kaggle.com/aashita/nyt-comments/data), set of readers' comments for articles published in the New York Times.\n",
    "\n",
    "## Overarching research question\n",
    "\n",
    "The comments provide a perspective to the kinds of concerns people raise in discussions related to online articles. The point of this example is to try to construct a set of keywords for dictionary search that captures the readers\n",
    "' views about the New York Times and journalistic reporting. Try iteratively different sets of keywords and examine how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(stringr)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify keywords for dictionary search\n",
    "keywords <- c(\"New York Times\", \"NYT\")\n",
    "keywords <- tolower( keywords )\n",
    "keywords <- paste( keywords, collapse = '|' ) ## this is a reqular experssion trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path <- 'data/nyt-comments/'\n",
    "files <- list.files( path ) ## see all files in directory\n",
    "files <- files[ grepl(\"Comments\", files) ]\n",
    "files <- paste( path, files, sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter <- 0\n",
    "comments <- 0\n",
    "\n",
    "# Iterate through the files and count comments which mention any of the keywords\n",
    "for( file in files ) {\n",
    "        \n",
    "    data <- read.csv( file )\n",
    "    \n",
    "    comments <- comments + nrow( data )\n",
    "    \n",
    "    commenttext <- data$commentBody\n",
    "    data$commentBodyLower <- tolower( commenttext )\n",
    "    \n",
    "    counter <- counter + sum( str_detect( data$commentBodyLower , keywords ) )\n",
    "    \n",
    "}\n",
    "\n",
    "print( paste( counter, \"/\", comments, \"comments mention any of the keywords:\", keywords, sep=\" \" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Try to think of other keywords that would make the search more comprehensive, while not producing excessive volumes of irrelevant material. Add these to the keywords above and rerun the script.\n",
    "* Are there any cases where the above code might not work? Modify the code to address these if possible.\n",
    "* The data has a `createDate` variable as well, which identifies when the comment was created. Based on this, try to look for some temporal trends in comment counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language analysis\n",
    "\n",
    "In many languages, different words can have different forms. For example, 'I have an apple' and 'I have several apples' convey almost the same information, similarly 'She had an apple' and 'She has an apple' are almost identical. In the Finnish language such cases abound, since words may have several forms thanks to the many suffixes.\n",
    "\n",
    "This complicates keyword-based analyses. One approach to reducing the complexity of language is to **stem** or **lemmatize** words into their basic form. Furthermore, tools such as the [NLP package](https://cran.r-project.org/web/packages/NLP/index.html) and various other [libraries](https://www.geeksforgeeks.org/natural-language-processing-with-r/) enable parsing text to identify proper nouns, named entities or to determine whether a word is an adjective, noun etc.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "The below code is a short example of stemming the words of a message. Replicate the keyword search above, but this time using proper stemmatization. Do the results change?\n",
    "\n",
    "After doing the search using stemming, try processing the resulting comments to create a Document-Term Matrix. You can also check the other code examples for hints on how this could be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(quanteda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message <- 'This is a longer example! Many words are included here, and we shall stem them all.'\n",
    "\n",
    "corp <- corpus(message)\n",
    "token <- tokens(corp)\n",
    "token <- tokens_wordstem( token )\n",
    "token"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
